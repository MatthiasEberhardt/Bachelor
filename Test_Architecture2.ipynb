{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import rasterio.warp\n",
    "from rasterio.plot import show\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "#from fastai.vision.image import *\n",
    "import fastbook\n",
    "import fastai\n",
    "fastbook.setup_book()\n",
    "from fastbook import *\n",
    "from fastai.vision.widgets import *\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import torch\n",
    "from torch.overrides import *\n",
    "from torch.nn.functional import *\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.nn import *\n",
    "#from IPython.core.debugg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\Almut Eberhardt\\Desktop\\Bachelor_Notebook\\Test_IM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied from https://github.com/fastai/fastai/blob/f633356359a29f8d869ce36659f7aa25660e946a/fastai/data/transforms.py#L230\n",
    "class Categorize_OneHot(DisplayedTransform):\n",
    "    \"Reversible transform of category string to `vocab` id\"\n",
    "    loss_func,order=CrossEntropyLossFlat(),1\n",
    "    def __init__(self, vocab=None, sort=True, add_na=False):\n",
    "        if vocab is not None: vocab = CategoryMap(vocab, sort=sort, add_na=add_na)\n",
    "        store_attr()\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None: self.vocab = CategoryMap(dsets, sort=self.sort, add_na=self.add_na)\n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, o):\n",
    "        try:\n",
    "            #change to one-hot-encoding\n",
    "            \n",
    "            length=len(self.vocab.o2i)\n",
    "            y_vec=torch.zeros(length)\n",
    "            y_vec[self.vocab.o2i[o]]=1\n",
    "            return TensorCategory(y_vec.type(torch.LongTensor))#TensorCategory(self.vocab.o2i[o])\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Label '{o}' was not included in the training dataset\") from e\n",
    "    #change\n",
    "    def decodes(self, o): \n",
    "        index=torch.argmax(o)\n",
    "        return Category(self.vocab[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=DataBlock(\n",
    "    blocks=(ImageBlock,TransformBlock(type_tfms=Categorize_OneHot(vocab=None, sort=True, add_na=False))),\n",
    "    #blocks=(ImageBlock,CategoryBlock),\n",
    "    #TransformBlock(type_tfms=Categorize_OneHot(vocab=None, sort=True, add_na=False))),\n",
    "    #TransformBlock(type_tfms=Categorize32f(vocab=None, sort=True, add_na=False))),\n",
    "    get_items=get_image_files,\n",
    "    splitter=RandomSplitter(valid_pct=0.2,seed=42),\n",
    "    get_y=parent_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls=images.dataloaders(path,num_workers=0,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Test_Net,self).__init__()\n",
    "        self.Conv1=nn.Conv2d(3,50,kernel_size=3,padding=1)\n",
    "        self.ReLU1=nn.ReLU()\n",
    "        self.Pool1=nn.MaxPool2d(kernel_size=2)\n",
    "        self.Conv2=nn.Conv2d(50,24,kernel_size=3,padding=1)\n",
    "        self.ReLU2=nn.ReLU()\n",
    "        self.Pool2=nn.MaxPool2d(kernel_size=2)\n",
    "        self.Conv3=nn.Conv2d(24,4,kernel_size=3,padding=1)\n",
    "        self.ReLU3=nn.ReLU()\n",
    "        self.Pool3=nn.MaxPool2d(kernel_size=2)\n",
    "        #print(type(self.Pool3))\n",
    "        self.Flat1=Flatten()\n",
    "        self.Dense1=nn.Linear(144,100)\n",
    "        self.ReLU4=nn.ReLU()\n",
    "        self.Dense2=nn.Linear(100,2)\n",
    "        self.ReLU5=nn.ReLU()\n",
    "        self.Softmax=nn.Softmax(dim=1)\n",
    "        #self.Flat2=Flatten()\n",
    "    def forward(self,x):\n",
    "        print(\"forward\")\n",
    "        res=self.Conv1(x)\n",
    "        res=self.ReLU1(res)\n",
    "        res=self.Pool1(res)\n",
    "        res=self.Conv2(res)\n",
    "        res=self.ReLU2(res)\n",
    "        res=self.Pool2(res)\n",
    "        res=self.Conv3(res)\n",
    "        res=self.ReLU3(res)\n",
    "        res=self.Pool3(res)\n",
    "        res=self.Flat1(res)\n",
    "        print(res.shape)\n",
    "        res=self.Dense1(res)\n",
    "        res=self.ReLU4(res)\n",
    "        res=self.Dense2(res)\n",
    "        res=self.Softmax(res)\n",
    "        #res=self.Flat2(res)\n",
    "        #res=self.ReLU5(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss_mod(_WeightedLoss):\n",
    "    r\"\"\"Creates a criterion that measures the Binary Cross Entropy\n",
    "    between the target and the output:\n",
    "\n",
    "    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n",
    "\n",
    "    .. math::\n",
    "        \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],\n",
    "\n",
    "    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n",
    "    (default ``'mean'``), then\n",
    "\n",
    "    .. math::\n",
    "        \\ell(x, y) = \\begin{cases}\n",
    "            \\operatorname{mean}(L), & \\text{if reduction} = \\text{'mean';}\\\\\n",
    "            \\operatorname{sum}(L),  & \\text{if reduction} = \\text{'sum'.}\n",
    "        \\end{cases}\n",
    "\n",
    "    This is used for measuring the error of a reconstruction in for example\n",
    "    an auto-encoder. Note that the targets :math:`y` should be numbers\n",
    "    between 0 and 1.\n",
    "\n",
    "    Notice that if :math:`x_n` is either 0 or 1, one of the log terms would be\n",
    "    mathematically undefined in the above loss equation. PyTorch chooses to set\n",
    "    :math:`\\log (0) = -\\infty`, since :math:`\\lim_{x\\to 0} \\log (x) = -\\infty`.\n",
    "    However, an infinite term in the loss equation is not desirable for several reasons.\n",
    "\n",
    "    For one, if either :math:`y_n = 0` or :math:`(1 - y_n) = 0`, then we would be\n",
    "    multiplying 0 with infinity. Secondly, if we have an infinite loss value, then\n",
    "    we would also have an infinite term in our gradient, since\n",
    "    :math:`\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\infty`.\n",
    "    This would make BCELoss's backward method nonlinear with respect to :math:`x_n`,\n",
    "    and using it for things like linear regression would not be straight-forward.\n",
    "\n",
    "    Our solution is that BCELoss clamps its log function outputs to be greater than\n",
    "    or equal to -100. This way, we can always have a finite loss value and a linear\n",
    "    backward method.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        weight (Tensor, optional): a manual rescaling weight given to the loss\n",
    "            of each batch element. If given, has to be a Tensor of size `nbatch`.\n",
    "        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
    "            the losses are averaged over each loss element in the batch. Note that for\n",
    "            some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
    "            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
    "            when reduce is ``False``. Default: ``True``\n",
    "        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
    "            losses are averaged or summed over observations for each minibatch depending\n",
    "            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
    "            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
    "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
    "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
    "            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where :math:`*` means, any number of additional\n",
    "          dimensions\n",
    "        - Target: :math:`(N, *)`, same shape as the input\n",
    "        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same\n",
    "          shape as input.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Sigmoid()\n",
    "        >>> loss = nn.BCELoss()\n",
    "        >>> input = torch.randn(3, requires_grad=True)\n",
    "        >>> target = torch.empty(3).random_(2)\n",
    "        >>> output = loss(m(input), target)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(BCELoss_mod, self).__init__(weight, size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        #print(input.shape)\n",
    "        #target=target[:,None]\n",
    "        target=target.type(torch.float)\n",
    "        #print(target.shape)\n",
    "        return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn=Test_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_mod(inp, targ, axis=-1):\n",
    "    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    #print(inp)\n",
    "    #print(targ.shape)\n",
    "    #pred,targ = flatten_check(inp.argmax(dim=axis), targ)\n",
    "    pred=inp.argmax(dim=axis)\n",
    "    targ=targ.argmax(dim=axis)\n",
    "    #print(pred)\n",
    "    return (pred == targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, tn, loss_func=BCELoss_mod(), metrics=accuracy_mod)\n",
    "#learn.model=learn.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_mod</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.694299</td>\n",
       "      <td>0.698448</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693210</td>\n",
       "      <td>0.683879</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.582953</td>\n",
       "      <td>0.295347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.319401</td>\n",
       "      <td>0.022572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.145535</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.065652</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.029993</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>0.003372</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(4, 144)\n",
      "forward\n",
      "(40, 144)\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
